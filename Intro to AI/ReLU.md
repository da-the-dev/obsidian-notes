**R**ectified **L**inear **U**nit (ReLU) is a type of [[Activation function]]:
![[Pasted image 20231204040619.png]]
where $u$ is an input to a [[neuron]]. This is one of the most popular activation functions.